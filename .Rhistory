dat <- mvrnorm(200,c(0,0,1,1),Sigma = sigmat)
if(!require(MASS)) install.packages("MASS"); require(MASS)
if(!require(lattice)) install.packages("lattice"); require(lattice)
if(!require(rasterVis)) install.packages("rasterVis"); require(rasterVis)
if(!require(psych)) install.packages("psych"); require(psych)
if(!require(pracma)) install.packages("psych"); require(pracma)
if(!require(nnls)) install.packages("nnls"); require(nnls)
if(!require(smacof)) install.packages("smacof"); require(smacof)
if(!require(deldir)) install.packages("deldir"); require(deldir)
if(!require(scales)) install.packages("scales"); require(scales)
if(!require(cluster)) install.packages("cluster"); require(cluster)
if(!require(ape)) install.packages("ape"); require(ape)
if(!require(pvclust)) {install.packages("pvclust"); require(pvclust)}
if (!require(mclust)) {install.packages("mclust"); require(mclust)}
install.packages("psych")
if(!require(nnls)) install.packages("nnls"); require(nnls)
if(!require(smacof)) install.packages("smacof"); require(smacof)
if(!require(deldir)) install.packages("deldir"); require(deldir)
if(!require(scales)) install.packages("scales"); require(scales)
if(!require(cluster)) install.packages("cluster"); require(cluster)
if(!require(ape)) install.packages("ape"); require(ape)
if(!require(pvclust)) {install.packages("pvclust"); require(pvclust)}
if (!require(mclust)) {install.packages("mclust"); require(mclust)}
set.seed(1) # set random seed to guarantee reproducible results
dat <- matrix(c(0,0,0,1,1,0),nrow=3,byrow=T) # generate triangle
plot(dat[,1],dat[,2],xlab="X",ylab="Y",
xlim=c(0,1.5),ylim=c(0,1.5),pch=20)
mat <- dist(dat) # calculate distances between three point
# the default metric is Euclidean
dmat # contains only nonredundant distance information
as.matrix(dmat) # can be converted to normal matrix with as.matrix()
# More exotic distance measures can be found in the 'proxy' package.
# The three most common are metrics in fMRI research are mean distance,
# Euclidean distance, and correlation distance. Only the Euclidean distance
# can be calculated directly through dist, but the other metrics are also
# straightforward to calculate. The following data will also demonstrate
# the essential difference between the metrics.
# data generation
set.seed(1)
sigmat <- matrix(c(1,0,.8,0,0,1,0,.8,.8,0,1,0,0,.8,0,1),nrow=4)
dat <- mvrnorm(200,c(0,0,1,1),Sigma = sigmat)
# mean distance
cmeans<-apply(dat,2,mean) # calculate means of each variable (object)
barplot(cmeans) # plot means
dmat1<-as.matrix(dist(cmeans)) # calculate distance between means
levelplot(dmat1) # heatmap of distances
# Euclidean distance
dmat2 <- as.matrix(dist(t(dat)))
levelplot(dmat2)
pairs(dat)
# correlation distance
dmat3 <- 1-cor(dat)
levelplot(dmat3)
# combined plot
dmat2<-dmat2/max(dmat2)
rlist <- list(raster(dmat1),raster(dmat2),raster(dmat3))
names(rlist)<-c("Mean","Euclidean","Correlation")
levelplot(stack(rlist),layout=c(3,1),at=seq(0,1,.01))
# As you can see, mean distance reflects means and means only. This is thus
# essentially equivalent to 'univariate' analyses in fMRI (though that
# analogy only applies exactly for ROIs or searchlights of size 1).
# Correlation distance doesn't reflect means at all. It only reflects the
# relative values of the objects' coordinates: e.g. are the same coordinates
# low or high in one object as in another? It also has the virtue of being
# scale-free (not demonstrated here) which means that units of measurement
# do not affect the results. Note that one can also calculate versions of
# correlation distance with other types of correlation (besides Pearson)
# such as Spearman rank correlation.
# Euclidean distance can be seen as an intermediate metric between mean
# and correlation distance. It is sensitive to both the means of data
# objects and to the patterns of 'ups and down' over their coordinates. In
# theory this might make Euclidean distance seem the most attractive, but
# in practice the means frequently dominate the pattern information and thus
# make correlation analysis more attractive.
# Note that for objects with a single coordinate (e.g. a pattern consisting
# of a single voxel) mean and Euclidean distance are identical (and also the
# same as 'absolute distance') and correlation distance is undefined.
# As with correlations in general, a relatively high number of observations
# must be available for reasonable results to accrue.
# Section 2: (Representational) similarity analysis -----------------------
# Representational similarity analysis is an increasingly popular form of
# confirmatory similarity-based analysis for fMRI (and other) data. It
# usually takes the form of correlation (or regression) between distance
# matrices. Since almost any type of data can be converted to distances
# regardless of dimensionalty, distribution, units, etc. this makes RSA a
# very flexible approach for testing hypothesis across different data
# sources (fMRI, behavior, computational models, etc.).
# Case study: mental state representation
# A cool example of R's IO capabilities - reading in directly from the web:
# load neural data (vectorized 60x60 correlation distance matrices, n=20)
ndat <- read.csv("http://markallenthornton.com/data/mentalstates/gcormat.csv",header=F)
# load rating data (PCA scores for 4 factors extracted from Mturk ratings)
rdat <- read.csv("http://markallenthornton.com/data/mentalstates/pc60.csv")
head(rdat) # print first 6 rows of dataframe
# Convert PCA dimensions into distances
rmat <- as.matrix(apply(rdat[,2:5],2,dist))
dat <- mvrnorm(200,c(0,0,1,1),Sigma = sigmat)
if(!require(MASS)) install.packages("MASS"); require(MASS)
if(!require(lattice)) install.packages("lattice"); require(lattice)
if(!require(rasterVis)) install.packages("rasterVis"); require(rasterVis)
if(!require(rasterVis)) install.packages("rasterVis"); require(rasterVis)
if(!require(psych)) install.packages("psych"); require(psych)
if(!require(pracma)) install.packages("psych"); require(pracma)
install.packages("psych")
pracma
install.packages("devtools")
devtools::install_github("jwdink/eyetrackingR")
library("eyetrackingR")
install_github("thohag/readSMI")
library(devtools)
install_github("thohag/readSMI")
version
update R
updateR()
version
library(ez)
install.packages("ez", repos=c("http://cran.rstudio.com"))
library(ez)
?ezANOVA
??ezANOVA
??ezANOVA
library(ez)
library(data.table)
library(ez)
library(pbkrtest)
install.packages(pbkrtest)
update.packages()
y
library(ez)
install.packages("pbkrtest")
version
install.packages("devtools")
devtools::install_github("CognitionOpenDataProject/CODreports")
install.packages("installr")
library(markdown)
res1
setwd("~/Dropbox (Personal)/Projects/KidSearch/Experiments/RAnalyses-OpenSource")
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(forcats) #manipulating factors in data frames
library(ez) # for anova
library(lme4)
library(markdown)
library("papaja")
## Step 2: Load data
exp = read_tsv("data/Animacy.txt")
#exp = read_tsv("data/ObjectSize.txt")
#exp = read_tsv("data/Edibility.txt")
str(exp)
exp$sub = as.factor(exp$sub)
exp$conditionNum = as.numeric(exp$condition)
exp$categoryNum = as.numeric(exp$categoryNum)
## Step 3: Preprocess data
# Count trials as sanity check
trialCount=exp %>%
group_by(sub) %>%
summarise(countTestTrials=sum(phase=='SixItems'))
# Only get data with correct trials less than 4 seconds long when there were six items on the screen.
exp.tidy=exp %>%
filter(RT<4000) %>%
filter(correct==1) %>%
filter(phase=='SixItems')
# Only include subjects who had more than 1 trial in each of 4 conditions
exp.tidy=exp.tidy %>%
group_by(sub) %>%
mutate(indivCountC1=sum(conditionNum==1 & categoryNum==1)) %>%
mutate(indivCountC2=sum(conditionNum==1 & categoryNum==2)) %>%
mutate(indivCountC3=sum(conditionNum==2 & categoryNum==1)) %>%
mutate(indivCountC4=sum(conditionNum==2 & categoryNum==2)) %>%
filter(indivCountC1>1) %>%
filter(indivCountC2>1) %>%
filter(indivCountC3>1) %>%
filter(indivCountC4>1)
# Store included subs
includedSubs=unique(exp.tidy$sub)
# Compute accuracy means
exp.accuracy=exp %>%
group_by(sub,conditionName, category) %>%
filter(phase=='SixItems') %>%
filter(is.element(sub,includedSubs)) %>% # only use subs included in RT anlayses for consistency
summarise(meanAcc = mean(correct)) # average wtihin subjects
trialCount
trialCount=exp %>%
group_by(sub) %>%
summarise(countTestTrials=sum(phase=='SixItems')) %>%
summarise(countSlowTrials=sum(RT>4000 & phase=='SixItems')) %>%
summarise(countIncorrectTrials=sum(correct==0 & phase=='SixItems'))
exp
exp$RT
trialCount=exp %>%
group_by(sub) %>%
summarise(countTestTrials=sum(phase=='SixItems')) %>%
summarise(countSlowTrials=sum(RT>4000 & phase=='SixItems')) %>%
summarise(countIncorrectTrials=sum(correct==0 & exp=='SixItems'))
trialCount
trialCount=exp %>%
group_by(sub) %>%
summarise(countTestTrials=sum(phase=='SixItems')) %>%
summarise(countSlowTrials=sum(RT>4000)) %>%
summarise(countIncorrectTrials=sum(correct==0 & exp=='SixItems'))
trialCount
trialCount=exp %>%
group_by(sub) %>%
summarise(countIncorrectTrials=sum(RT>4000 & phase=='SixItems'))
trialCount
trialCount=exp %>%
group_by(sub) %>%
summarise(countSlowTrials=sum(RT>4000 & phase=='SixItems'))
trialCount
trialCount$countSlowTrials
trialCount$testTrials=exp %>%
group_by(sub) %>%
summarise(countTestTrials=sum(phase=='SixItems')) %>%
trialCount$SlowTrials=exp %>%
group_by(sub) %>%
summarise(countSlowTrials=sum(RT>4000 & phase=='SixItems'))
trialCount$countErrors=exp %>%
group_by(sub) %>%
summarise(countIncorrectTrials=sum(RT>4000 & phase=='SixItems'))
trialCount
trialCount=exp %>%
group_by(sub) %>%
summarise(countTestTrials=sum(phase=='SixItems')) %>%
summarise(countSlowTrials=sum(RT>4000 & phase=='SixItems'))
trialCount=exp %>%
group_by(sub) %>%
summarise(trials=sum(phase=='SixItems')) %>%
slowCount=exp %>%
group_by(sub) %>%
summarise(slow=sum(RT>4000 & phase=='SixItems'))
errorCount=exp %>%
group_by(sub) %>%
summarise(errors=sum(correct==0 & phase=='SixItems'))
v
trialCount=exp %>%
group_by(sub) %>%
summarise(trials=sum(phase=='SixItems'))
trialCount
trialCount$trials
ttrialCount$trials
slowCount=exp %>%
group_by(sub) %>%
summarise(slow=sum(RT>4000 & phase=='SixItems'))
errorCount=exp %>%
group_by(sub) %>%
summarise(errors=sum(correct==0 & phase=='SixItems'))
slowCount$slow
slowCount$slow/trialCount$trials
mean(slowCount$slow/trialCount$trials)
round(mean(slowCount$slow/trialCount$trials),2)
slowCount=exp %>%
group_by(sub) %>%
summarise(slow=sum(RT>4000 & phase=='SixItems' & correct==1))
round(mean(slowCount$slow/trialCount$trials),2)
round(mean(slowCount$slow/trialCount$trials),2)
slowCount
slowCount
slowCount$slow
slowCount$slow(includedSubs)
slowCount$sub
is.element(slowCount$sub,includedSubs)
slowCount$slow
median(slowCount$slow)
mean(slowCount$slow)
(slowCount$slow)
(slowCount$slow)/trialCount$trials
mean((slowCount$slow)/trialCount$trials)
errorCount
errorCount$errors
errorCount$errors/trialCount$trials
mean(errorCount$errors/trialCount$trials)
round(mean(errorCount$errors/trialCount$trials)*100,2)
round(mean(slowCount$slow/trialCount$trials)*100,2)
trialCountFiltered=exp.tidy %>%
group_by(sub,condition) %>%
filter(is.element(sub,includedSubs)) %>%
summarise(condCount=sum(conditionNum==1)) %>%
summarise(condCount=sum(conditionNum==2)) %>%
trialCount
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(forcats) #manipulating factors in data frames
library(ez) # for anova
library(lme4)
library(markdown)
library("papaja")
## Step 2: Load data
exp = read_tsv("data/Animacy.txt")
#exp = read_tsv("data/ObjectSize.txt")
#exp = read_tsv("data/Edibility.txt")
str(exp)
exp$sub = as.factor(exp$sub)
exp$conditionNum = as.numeric(exp$condition)
exp$categoryNum = as.numeric(exp$categoryNum)
## Step 3: Preprocess data
# count how many trials per subject
trialCount=exp %>%
group_by(sub) %>%
summarise(trials=sum(phase=='SixItems'))
# count how many errors per subject
errorCount=exp %>%
group_by(sub) %>%
summarise(errors=sum(correct==0 & phase=='SixItems'))
# count how many slow, correct trials per subject
slowCount=exp %>%
group_by(sub) %>%
summarise(slow=sum(RT>4000 & phase=='SixItems' & correct==1))
# what percentage of trials were incorrect?
round(mean(errorCount$errors/trialCount$trials)*100,2)
# what percentage of correct trials were slow?
round(mean(slowCount$slow/trialCount$trials)*100,2)
# Only get data with correct trials less than 4 seconds long when there were six items on the screen.
exp.tidy=exp %>%
filter(RT<4000) %>%
filter(correct==1) %>%
filter(phase=='SixItems')
# Now only include subjects who had more than 1 trial speeded trial in each of 4 conditions
exp.tidy=exp.tidy %>%
group_by(sub) %>%
mutate(indivCountC1=sum(conditionNum==1 & categoryNum==1)) %>%
mutate(indivCountC2=sum(conditionNum==1 & categoryNum==2)) %>%
mutate(indivCountC3=sum(conditionNum==2 & categoryNum==1)) %>%
mutate(indivCountC4=sum(conditionNum==2 & categoryNum==2)) %>%
filter(indivCountC1>1) %>%
filter(indivCountC2>1) %>%
filter(indivCountC3>1) %>%
filter(indivCountC4>1)
# Store included subs
includedSubs=unique(exp.tidy$sub)
# How many trials are included in each condition in this subset of children?
trialCountIncludedSubs=exp.tidy %>%
group_by(sub,condition) %>%
filter(is.element(sub,includedSubs)) %>%
summarise(allTrials=length(RT)) %>%
summarise(uniformCount=sum(conditionNum==1)) %>%
summarise(mixedCount=sum(conditionNum==2))
# Compute accuracy means
exp.accuracy=exp %>%
group_by(sub,conditionName, category) %>%
filter(phase=='SixItems') %>%
filter(is.element(sub,includedSubs)) %>% # only use subs included in RT anlayses for consistency
summarise(meanAcc = mean(correct)) # average wtihin subjects
# summarize accuracy results
condAccMeans=exp.accuracy %>%
group_by(conditionName) %>%
summarize(avgCondAcc=mean(meanAcc), sdCondRT=sd(meanAcc))
# descriptive means (mixed v uniform)
UniMixAcc=round(condAccMeans$avgCondAcc*100,2)
# ANOVA
accuracyResults=ezANOVA(dv= .(meanAcc), wid= .(sub), within= .(conditionName, category), detailed=TRUE, data=data.frame(exp.accuracy), type=3)
partialEtaSquared=accuracyResults$ANOVA$SSn/(accuracyResults$ANOVA$SSn + accuracyResults$ANOVA$SSd)
trialCountIncludedSubs=exp.tidy %>%
group_by(sub,condition) %>%
filter(is.element(sub,includedSubs)) %>%
summarise(allTrials=length(RT)) %>%
summarise(uniformCount=sum(conditionNum==1)) %>%
summarise(mixedCount=sum(conditionNum==2))
exp.tidy
exp.tidy$conditionNum
trialCountIncludedSubs=exp.tidy %>%
group_by(sub,conditionNum) %>%
filter(is.element(sub,includedSubs)) %>%
summarise(allTrials=length(RT)) %>%
summarise(uniformCount=sum(conditionNum==1)) %>%
summarise(mixedCount=sum(conditionNum==2))
exp.tidy$conditionNum
sum(conditionNum==1)
sum(exp.tidy$conditionNum==1)
trialCountIncludedSubs=exp.tidy %>%
group_by(sub,conditionNum) %>%
filter(is.element(sub,includedSubs)) %>%
summarise(allTrials=length(RT)) %>%
summarise(uniformCount=sum(conditionNum==1)) %>%
summarise(mixedCount=sum(conditionNum==2))
trialCountIncludedSubs=exp.tidy %>%
group_by(sub,conditionNum) %>%
filter(is.element(sub,includedSubs)) %>%
summarise(allTrials=length(RT)) %>%
summarise(uniformCount=sum(conditionNum==1)) %>%
summarise(mixedCount=sum(conditionNum==2))
trialCountIncludedSubs=exp.tidy %>%
group_by(sub,conditionNum) %>%
filter(is.element(sub,includedSubs)) %>%
summarise(uniformCount=sum(conditionNum==1)) %>%
summarise(mixedCount=sum(conditionNum==2))
trialCountIncludedSubs
summarise(uniformCount=sum(conditionNum==1), mixedCount=sum(conditionNum==2)) %>%
trialCountIncludedSubs=exp.tidy %>%
group_by(sub,conditionNum) %>%
filter(is.element(sub,includedSubs)) %>%
summarise(uniformCount=sum(conditionNum==1), mixedCount=sum(conditionNum==2)) %>%
trialCountIncludedSubs=exp.tidy %>%
group_by(sub,conditionNum) %>%
filter(is.element(sub,includedSubs)) %>%
summarise(uniformCount=sum(conditionNum==1), mixedCount=sum(conditionNum==2))
trialCountIncludedSubs
trialCountIncludedSubs=exp.tidy %>%
group_by(sub) %>%
filter(is.element(sub,includedSubs)) %>%
summarise(uniformCount=sum(conditionNum==1), mixedCount=sum(conditionNum==2))
trialCountIncludedSubs
trialCountIncludedSubs$uniformCount
mean(trialCountIncludedSubs$uniformCount)
mean(trialCountIncludedSubs$mixedCount)
round(mean(trialCountIncludedSubs$uniformCount),2)
round(mean(trialCountIncludedSubs$mixedCount),2)
round(mean(trialCountIncludedSubs$uniformCount),2)
round(mean(trialCountIncludedSubs$mixedCount),2)
round(mean(trialCountIncludedSubs$uniformCount),2)+round(mean(trialCountIncludedSubs$mixedCount),2)
trialCountIncludedSubs=exp.tidy %>%
group_by(sub) %>%
filter(is.element(sub,includedSubs)) %>%
summarise(trialCount=length(RT), uniformCount=sum(conditionNum==1), mixedCount=sum(conditionNum==2))
trialCountIncludedSubs
round(mean(trialCountIncludedSubs$trialCount),2)
round(sd(trialCountIncludedSubs$trialCount),2)
Children were `round(UniMixAcc[1],2)`% accurate on uniform trials, and  `round(UniMixAcc[2],2)` on mixed category trials.
Children were `round(UniMixAcc[1],2)`% accurate on uniform trials, and  `round(UniMixAcc[2],2)` on mixed category trials.
? apa_print
RTResults=ezANOVA(dv= .(RT), wid= .(sub), within= .(conditionName, category), detailed=TRUE, data=data.frame(exp.tidy), type=3)
summary(RTResults)
apa_print(summary(RTResults))
? apa_print.anova
apa_print.aov(RTResults)
print_anova(RTResults)
RTResults
RTResults$ANOVA$DFn,RTResults$ANOVA$DFd
print(RTResults)
source('~/.active-rstudio-document', echo=TRUE)
condMeans
UniMixAcc
condAccMeans
UniMixAcc
categAccMeans=exp.accuracy %>%
group_by(category) %>%
summarize(avgCondAcc=mean(meanAcc), sdCondRT=sd(meanAcc))
categAccMeans
AnimObjAcc=round(categAccMeans$avgCondAcc*100,2)
AnimObjAcc=round(categAccMeans$avgCondAcc*100,2)
AnimObjAcc
accuracyResults=ezANOVA(dv= .(meanAcc), wid= .(sub), within= .(conditionName, category), detailed=TRUE, data=data.frame(exp.accuracy), type=3)
accuracyResults
accuracyResults$ANOVA$p
accuracyResults$ANOVA$p[1]
round(accuracyResults$ANOVA$p[1],2)
round(accuracyResults$ANOVA$p[1],3)
round(accuracyResults$ANOVA$p[1],4)
round(accuracyResults$ANOVA$p[1],5)
round(accuracyResults$ANOVA$p[1],20)
round(accuracyResults$ANOVA$p[1],1)
round(accuracyResults$ANOVA$p[1],2)
round(accuracyResults$ANOVA$p[1],8)
round(accuracyResults$ANOVA$p[1])
accuracyResults$ANOVA$p[1]
accuracyResults
accuracyResults$ANOVA$p[2]
round(accuracyResults$ANOVA$p[2])
round(accuracyResults$ANOVA$p[2],2)
round(accuracyResults$ANOVA$F[2],2)
round(accuracyResults$ANOVA$p[2],2)
partialEtaSquared
partialEtaSquared[2]
partialEtaSquared=accuracyResults$ANOVA$SSn/(accuracyResults$ANOVA$SSn + accuracyResults$ANOVA$SSd)
partialEtaSquared
round(partialEtaSquared[2],2)
F(1,13)= `round(accuracyResults$ANOVA$F[2],2)`, *p*=round(accuracyResults$ANOVA$p[2],2), round(partialEtaSquared[2],2)
F(1,13)= `round(accuracyResults$ANOVA$F[2],2)` *p*=round(accuracyResults$ANOVA$p[2],2), round(partialEtaSquared[2],2)
F(1,13)= `round(accuracyResults$ANOVA$F[2],2)` *p*=round(accuracyResults$ANOVA$p[2],2),
print(F(1,13)= `round(accuracyResults$ANOVA$F[2],2)` *p*=round(accuracyResults$ANOVA$p[2],2)) round(partialEtaSquared[2],2)
print(F(1,13)= `round(accuracyResults$ANOVA$F[2],2)` *p*=round(accuracyResults$ANOVA$p[2],2)) round(partialEtaSquared[2],2)
round(categoryMeans$avgCatRT,0)
round(categoryMeans$avgCatRT,0)
categoryMeans
round(categoryMeans[1],0)
round(categoryMeans$avgCatRT[1],0)
round(categoryMeans$avgCatRT[2],0)
source('~/.active-rstudio-document', echo=TRUE)
CategoryAccs
categAccMeans=exp.accuracy %>%
source('~/.active-rstudio-document', echo=TRUE)
setwd("~/Dropbox (Personal)/Projects/KidSearch/Experiments/RAnalyses-OpenSource")
source('~/.active-rstudio-document', echo=TRUE)
CategoryAccs
categAccMeans
categAccMeans$category
categAccMeans$category[1]
CategoryRT
categoryMeans
source('~/.active-rstudio-document', echo=TRUE)
length(exp.RT)
exp
condAccMeans
length(includedSubs)
accuracyResults
RTResults$ANOVA$Dfd[2]
RTResults$ANOVA$DFd[2]
head(exp)
partialEtaSquared
partialEtaSquared[3]
avgCondAcc
source('~/.active-rstudio-document', echo=TRUE)
condMeans
condAccMeans
condAccMeans$conditionName
condAccMeans$conditionName[1]
accuracyResults
